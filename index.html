---
layout: default
title: Home
---

<div class="pages">
  
<!--   <div class="section">
    <h2>About</h2>
    <p>
    </p>
  </div> -->
  
  <div class="section">
    <h2>Publications</h2>

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/beyondastar.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, Yuandong Tian<br>
          <b><a href="https://arxiv.org/abs/2402.14083">Beyond A&#8727;: Better Planning with Transformers via Search Dynamics Bootstrapping</a></b><br>
          <i>arXiv: 2402.14083 [cs.AI], 2024 </i>
        </p>
      </div>
    </div>

    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/h_tdmpc.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Rohan Chitnis, Yingchen Xu, Bobak Hashemi, Lucas Lehnert, Urun Dogan, Zheqing Zhu, Olivier Delalleau<br>
          <b><a href="https://arxiv.org/abs/2306.00867">IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control</a></b><br>
          <i>arXiv: 2306.00867 [cs.LG], 2023 [to appear at ICRA 2024]</i>
        </p>
      </div>
    </div>

    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/etapsilearning.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Arnav Kumar Jain, Lucas Lehnert, Irina Rish, Glen Berseth<br>
          <b><a href="https://papers.nips.cc/paper_files/paper/2023/hash/9c7900fac04a701cbed83256b76dbaa3-Abstract-Conference.html">Maximum State Entropy Exploration using Predecessor and Successor Representations</a></b><br>
          <i>Advances in Neural Information Processing Systems 36 pre-proceedings (NeurIPS 2023)</i>
          <a href="https://arxiv.org/abs/2306.14808">[arXiv]</a>
          <a href="https://github.com/arnavkj1995/Eta_Psi_Learning">[Code]</a>
        </p>
      </div>
    </div>

    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/reward_predictive_cluster_tree.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert, Michael J. Frank, and Michael L. Littman<br>
          <b><a href="https://arxiv.org/abs/2211.03281">Reward-predictive clustering</a></b><br>
          <i>arXiv: 2211.03281 [cs.LG], 2022</i> 
        </p>
      </div>
    </div>

    <div class="h-line" />
    
    <div class="publication">
      <div class="publication-img">
        <img src="public/images/RewardPredictiveGuitar.png" /> 
      </div>
      <div class="publication-ref">
      <p>
        Lucas Lehnert<br>
        <b><a href="http://cs.brown.edu/research/pubs/theses/phd/2021/lehnert.lucas.pdf">Encoding Reusable Knowledge in State Representations</a></b><br>
        <i>PhD Dissertation, Brown University, 2021</i>
      </p>
      </div>
    </div>

    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/HistogramPredictive.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert, Michael L. Littman, and Michael J. Frank<br>
          <b><a href="https://doi.org/10.1371/journal.pcbi.1008317">Reward-predictive representations generalize across tasks in reinforcement learning</a></b><br>
          <i>PLOS Computational Biology, 2020</i>
          <a href="https://github.com/lucaslehnert/rewardpredictive">[Code]</a>
          <a href="https://hub.docker.com/r/lucasdocker/rewardpredictive">[Docker Hub]</a>
          <a href="https://www.biorxiv.org/content/10.1101/653493v3">[bioRxiv]</a>
        </p>
      </div>
    </div>

    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/RewardPredictive.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert and Michael L. Littman<br>
          <b><a href="https://jmlr.csail.mit.edu/papers/v21/19-060.html">Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning</a></b><br>
          <i>Journal of Machine Learning Research (JMLR), 2020</i>
          <a href="https://arxiv.org/pdf/1901.11437.pdf">[arXiv]</a>			
        </p>
          
        <p>  
          Lucas Lehnert and Michael L. Littman<br>
          <b><a href="https://arxiv.org/pdf/1807.01736.pdf">Transfer with Model Features in Reinforcement Learning</a></b><br>
          <i>Lifelong Learning: A Reinforcement Learning Approach workshop at FAIM, Stockholm, Sweden, 2018</i> [<a href="https://arxiv.org/abs/1807.01736">arXiv</a>]
        </p>
      </div>
    </div>
    
    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/PACAbstraction.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          David Abel, Dilip S. Arumugam, Lucas Lehnert, and Michael L. Littman<br>
          <b><a href="http://proceedings.mlr.press/v80/abel18a.html">State Abstractions for Lifelong Reinforcement Learning</a></b><br>
          <i>Proceedings of the 35th International Conference on Machine Learning, PMLR 80:10-19, 2018</i> [<a href="http://proceedings.mlr.press/v80/abel18a/abel18a.pdf">PDF</a>]
        </p>
          
        <p>  
          David Abel, Dilip Arumugam, Lucas Lehnert, and Michael L. Littman<br>
          <b><a href="https://drive.google.com/file/d/1MrNzgpVXYRlp8xfQ6Qjmxs7UNO5T376g/view">Toward Good Abstractions for Lifelong Learning</a></b><br>
          <i>NIPS workshop on Hierarchical Reinforcement Learning, 2017</i> [<a href="public/documents/NIPS2017_HRL_abstraction.pdf">PDF</a>]
        </p>
      </div>
    </div>

    <div class="h-line" />
    
    <div class="publication">
      <div class="publication-img">
        <img src="public/images/LHP.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert, Romain Laroche, and Harm van Seijen<br>
          <b><a href="https://doi.org/10.1609/aaai.v32i1.11646">On Value Function Representation of Long Horizon Problems</a></b><br>
          <i>In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018</i> [<a href="https://doi.org/10.1609/aaai.v32i1.11646">PDF</a>]
        </p>
      </div>
    </div>

    <div class="h-line" />
    
    <div class="publication">
      <div class="publication-img">
        <img src="public/images/SFNav.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert, Stefanie Tellex, and Michael L. Littman<br>
          <b><a href="https://drive.google.com/file/d/0B9dqzboiV5u-dUlLSmRxTzdhXzQ/view">Advantages and Limitations of Using Successor Features for Transfer in Reinforcement Learning</a></b><br>
          <i>Lifelong  Learning:  A Reinforcement Learning Approach workshop @ICML, Sydney,  Australia,  2017</i> [<a href="https://arxiv.org/pdf/1708.00102.pdf">arXiv</a>]<br>
          <i>Best Student Paper Award</i>
        </p>
      </div>
    </div>

    <div class="h-line" />
    
    <div class="publication">
      <div class="publication-img">
        <img src="public/images/MSPBE.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert and Doina Precup<br>
          <b><a href="https://ewrl.files.wordpress.com/2016/11/ewrl13-2016-submission_24.pdf">Using Policy Gradients to Account for Changes in Behavior Policies Under Off-policy Control</a></b><br>
          <i>The 13th European Workshop on Reinforcement Learning (EWRL 2016)</i> [<a href="public/documents/ewrl13-2016-submission_24.pdf">pdf</a>]
        </p>

        <p>  
          Lucas Lehnert and Doina Precup<br>
          <b><a href="https://arxiv.org/pdf/1512.04105v1.pdf">Policy Gradient Methods for Off-policy Control</a></b><br>
          <i>arXiv: 1512.04105 [cs.AI], 2015</i> 
        </p>
          
        <p> 
          Lucas Lehnert<br>
          <b><a href="http://digitool.library.mcgill.ca/R/-?func=dbin-jump-full&object_id=145481&silo_library=GEN01">Off-policy control under changing behaviour</a></b><br>
          <i>Master of Science Thesis, McGill University, 2017</i> [<a href="public/documents/MScThesis.pdf">pdf</a>]
        </p>
      </div>
    </div>

    <div class="h-line" />

    <div class="publication">
      <div class="publication-img">
        <img src="public/images/RobotPath.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Lucas Lehnert and Doina Precup<br>
          <b><a href="http://www.ias.informatik.tu-darmstadt.de/uploads/ALR2014/Lehnert_ALR2014.pdf">Building a Curious Robot for Mapping</a></b><br>
          <i>Autonomously Learning Robots workshop (ALR 2014), NIPS 2014</i> [<a href="public/documents/Lehnert_ALR2014.pdf">pdf</a>]
        </p>
      </div>
    </div>

    <div class="h-line" />
    
    <div class="publication">
      <div class="publication-img">
        <img src="public/images/HeartMRI.png" /> 
      </div>
      <div class="publication-ref">
        <p>
          Arthur Mensch, Emmanuel Piuze, Lucas Lehnert, Adrianus J. Bakermans, Jon Sporring, Gustav J. Strijkers, and Kaleem Siddiqi<br>
          <b><a href="http://link.springer.com/chapter/10.1007/978-3-319-14678-2_9">Connection Forms for Beating the Heart. Statistical Atlases and Computational Models of the Heart - Imaging and Modelling Challenges</a></b><br>
          <i>8896: 83-92. Springer International Publishing, 2014</i>
        </p>
      </div>
    </div>

    <div class="h-line" />
  </div>
  
  <div class="section">
    <h2>Talks</h2>

    <p>
      <b>Encoding Reusable Knowledge in State Representations</b><br>
      <i>Invited talk at <a href="https://sites.google.com/lisa.iro.umontreal.ca/tea-talks/home">Mila Tea Talks, Mila - Quebec Artificial Intelligence Institute</a>, Montr&eacute;al, Canada, 2020</i> [<a href="https://bluejeans.com/playback/s/XOGHptVKVSWfxzeoTLE0Q8pqZ7YXE5SfzYkeAs9b3njZDpKrh3fYoaK8JKDZXHc0">recording</a>]
    </p>

    <p>
      <b>Should intelligent agents learn how to behave optimally or learn how to predict future outcomes?</b><br>
      <i>Invited talk at <a href="https://sites.google.com/view/rldm-serl-2019/home">Structure for Efficient Reinforcement Learning (SERL)</a> at RLDM 2019, Montr&eacute;al, Canada, 2019</i>
    </p>

    <p>
      <b>Transfer Learning Using Successor State Features</b><br>
      <i>Invited talk at the workshop <a href="https://rllabmcgill.github.io/icml2017-rlworkshop/">ICML’2017 RL late breaking results event</a>, at ICML, Sydney, Australia, 2017</i> [<a href="public/documents/RL_Workshop_SF_ICML_2017.pdf">slides</a>]
    </p>
  </div>

<!-- <div class="pages">
  {% for page in paginator.pages %}
  <div class="page">
    <h1 class="page-title">
      {{ page.title }}
    </h1>
    {{ page.content }}
  </div>
  {% endfor %}
</div>  -->

<!-- <div class="pagination">
  {% if paginator.next_page %}
    <a class="pagination-item older" href="{{ pages.baseurl }}page{{paginator.next_page}}">Older</a>
  {% else %}
    <span class="pagination-item older">Older</span>
  {% endif %}
  {% if paginator.previous_page %}
    {% if paginator.page == 2 %}
      <a class="pagination-item newer" href="{{ site.baseurl }}">Newer</a>
    {% else %}
      <a class="pagination-item newer" href="{{ site.baseurl }}page{{paginator.previous_page}}">Newer</a>
    {% endif %}
  {% else %}
    <span class="pagination-item newer">Newer</span>
  {% endif %}
</div>  -->
